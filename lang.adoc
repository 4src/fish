= SE+AI, just the important bits
:Author:    Tim Menzies
:Email:     timm@ieee.org
:Date:      July 26,2023
:Revision:  v0.1
:toc: left
:toclevels: 5
:icons: font
:sectanchors: 
:url-repo: https://my-git-repo.com 
:stem: latexmath


:fn-druzdel:     footnote:[Druzdzel, Marek J. "Some properties of joint probability distributions." Uncertainty Proceedings 1994. Morgan Kaufmann, 1994. 187-194.]
:fn-hall03:      footnote:[Mark A. Hall, Geoffrey Holmes: Benchmarking Attribute Selection Techniques for Discrete Class Data Mining. IEEE Trans. Knowl. Data Eng. 15(6): 1437-1447 (2003)]
:fn-lace2:       footnote:[F. Peters, T. Menzies and L. Layman, "LACE2: Better Privacy-Preserving Data Sharing for Cross Project Defect Prediction," 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Florence, Italy, 2015, pp. 801-811, doi: 10.1109/ICSE.2015.92.]
:fn-lopez10:     footnote:[Olvera-López, J.A., Carrasco-Ochoa, J.A., Martínez-Trinidad, J.F. et al. A review of instance selection methods. Artif Intell Rev 34, 133–143 (2010). https://doi.org/10.1007/s10462-010-9165-y]
:fn-noclass:     footnote:[https://youtu.be/o9pEzgHorH0, Stop Writing Classes, Jack Diederich, Mark 15, 2012]
:fn-notrational: footnote:[For an illuminating, and perhaps humbling experience, just look at "list of human cognitive biases" in Wikipedia.  At last count, 125 (and growing). This page lists all the ways humans routinely act in non-rational manner.]
:fn-ribbon:      footnote:[https://www.neowin.net/news/building-windows-8-the-new-ribbon-based-explorer-explained/[Building Windows 8: The new ribbon-based Explorer]]
:fn-sample:      footnote:[Approximate a larger population on characteristics relevant to the research question, to be representative so that researchers can make inferences about the larger population. From the https://www.nlm.nih.gov/nichsr/stats_tutorial/section2/mod1_sampling.html[National Library of Medicine]]
:fn-trivial:     footnote:[https://en.wikipedia.org/wiki/Law_of_triviality]
:fn-unit:        footnote:[https://en.wikipedia.org/wiki/Unit_sphere]
:fn-yu:          footnote:[Yu, Z., Theisen, C., Williams, L., & Menzies, T. (2019). Improving vulnerability inspection efficiency using active learning. IEEE Transactions on Software Engineering, 47(11), 2401-2420.]

== Cut the Crap

[quote,Michelangelo di Lodovico Buonarroti Simoni]
"The sculpture is already complete within the marble block, before
I start my work. It is already there, I just have to chisel away
the superfluous material."

[quote,Me]
"The best thing you can do with most of what you have, is to throw it away."

Sadly, all too often,  simple things get too complicated.  I'm a
knowledge engineer who writes software systems to capture human
ideas.  If I ask any subject matter experts about, well,  anything
at all, they feel duty bound to say something- even if it is outside
their expertise and experience.  As a result, software specifications
can collect much silly stuff.

The same thing happens with committees.  Parkinson's law of
Triviality (also know as _the bike-shed effect_{fn-trivial} says
that groups spend a disproportionate amount of their time on menial
and trivial matters while the important stuff gets unattended.
For example, when designing a nuclear power plant, a committee might
spend days debating  the bike sheds but only a few hours discussing
how to dispose of the spent nuclear fuel rods. 

As with human discussions, so to with software. Take any software
system and look into how much of it actually needs to be used (hint: not much):

[grid=rows]
.Table Software usually runs  in  a small corner of the total space
|===
|When Microsoft instrumented their Office
suite and found that the top 10-most-used buttons
covered 82% of the command use in that tools
(and some of the  rest were used vanishingly rarely{fn-ribbon}.

|When Jack
Diederich carefully rewrote commonly-used
Python libraries (just focusing on the parts he needed) 10,000 lines of
code reduced by 99% to  just 135 lines{fn-noclass}. 

| When Marek Druzdel  tracked the used states within his intensive care software
(which was a system with over 500,000 possible states), he found
that the top (1,  11,  40) most used states
where seen (52%, 75%, 91%) of the time{fn-druzdel}.
|===



Another way to reflect on the effective size of software
is to
first collect data from that software then see how much
that that can be compressed (without loss of signal). The experience to
date is that those compression rates can be very large.

[grid=rows]
.Table Software data can be dramatically compressed
|===
| For tabular data (where one column is the goal), instance and column ranking
algorithms can select around 33% (or less) of the columns 
(that are most
associated with the goal), and around
10% of the rows (that can serve as representatives for the rest)
Such column and instance selection
can shrink tables of software data  to just _.33*.1 = 3%_ of their original size
{fn-lopez10} {fn-hall03}.

| The models learned in this tiny corner of the data works just as well
as anything else. Better yet, this reduced space is better for data sharing since
anything that was private in 97%   of the data is now removed {fn-lace2}.

| Similar compressions have been seen with support vector
algorithms.
Support vectors are the "border guards" that mark out the boundary
between _this_ kind of stuff and _that_ kind of stuff. 
Once that boundary is known,
we can forget the rest of the data since (for classification tasks)
all we need to know is where we stand with
respect to the border.
Zhe Yu and his colleagues report that when their tools read 20,000+
functions from the Firefox blowers, around 200 vectors were enough
to distinguish the functions with potential security flaws {fn-yu}.
|===

There is some maths suggesting that the above is not unaccepted. The maths
is a little esoteric but it comes down to a curiosity about high-dimensional data.
Let's ask what is the value of have more data; specifically,
it is easier to learn a model from a table of data with stem:[n\in \{5,10,20\}]
attributes.
To build any part of a model,
we need to find several similar data points that can confirm the presence of that part.
If we say each  attribute is one dimension, then  the space of data similar to any one example
is a sphere of volume 
stem:[V_n=V_{n-2}\frac{2{\pi}r}{n}]. 
That is, we can compute
the volume of a four-dimensional sphere by looking back at the volume
of a two-dimensional sphere (which is 
stem:[V_2=2{\pi}r]), then multiplying by a factor stem:[\frac{2{\pi}r}{n}] {fn-unit}.
So, if we were to hunt for similar examples in the unit sphere, what is the space
where they can be found:

- stem:[V_5=5.25]
- stem:[V_{10}=2.55]
- stem:[V_{20}=0.03]

Note that our sphere is _shrinking_
as the number of dimensions gets large.
See, for a unit-sphere (where stem:[r=1]), the factor stem:[\frac{2{\pi}r}{n}] 
becomes less that one when
stem:[n>2\pi]; i.e. when stem:[n>6]. To say that another away:
if we over-elaborate our models with too
many variables, we will not be able to find the data to 
validate those models.

(Aside: there are some caveats here. Each  dimension  could be itself be some 
synthesized construct that combines multiple dimensions-- in which case we can model
more that stem:[n=6] dimensions. Also, even if the _current model_ needs just a few attributes,
whose to know when your context switches and suddenly you are in a new situation that
needs a new model with six different attributes?)  
 

at what happens when we try to learn a model using lots of data,
versus learning from just a little. For example, we have a spreadsheet with 10 columns
or 100.
If you want to find relevant data to elan some part of a model,  you should look
to  the sphere of similar things that a close by.
Now this sphere for  has a volume that is can be calulated i.e. things that are close-by. One way to model "close"
is to reflect on the 
[[first,First Steps]]
== First Steps with http://asciidoc.org[AsciiDoc]

[plantuml, format=svg, opts="inline"]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
Root
|_ Element 1
  |_ Element 1.1
  |_ Element 1.2
|_ Element 2
  |_ Element 2.1
end legend
----

We all know how complex and difficult
and expensive it can be to build and maitntain
and understand  
complex software and AI software can be. But not everything
is that hard. Suppose we ask what is the _least_ we can do to
achieve the _most_, what do we get? Now  that we have decades
of experience with AI and SE, what have we learned?
What are the big ideas that lead to the smallest scripts
that do the most?

[plantuml, asciidoc-plant-uml-sample, svg]
-------------------------------------------
class A<T> <<singleton>> {
    {abstract} -int x
    {static} #void meta()
}

class B

A <|-- B
-------------------------------------------

One big idea, that is often missing is "sampling".
Sampling has a formal definition(footnote{fn-sample}) but I like to call it
"not looking at everything". Life is short, the road is long,
you stop to look at everything or you will never get 
anything done(footnote{fn-maths}).

:fn-maths: footnote:[The are  (stem:[10^{24}]) stars in the observable univerise. 
Say you build software with variables that take four values: 
veryLow, low, hi, veryHigh_. If you software has more that four dozen
variable variables then your software has more states stars
in the sky (stem:[4^{48} > 10^{28}]).]

sampling take sus to confngituve sphyctologu and all the quirks
of the human brain that mean we rountinely do not behave
rationally(footnote{notrational}). adn from there, its just
a quick jump for quirks in one brain, to all the qurijly things
that happen when groups of brains get together. Human herd behaviour is fascinating,
if not a little scary.

ai softare is softare. softwre has bugs. things hwith bugs have 
to be tested. 
But how to test softare, which is a very diverse set of artifacts?
In a cost-effective way? I say the answer is modeling. I say that
when faced with somethig u dont understad

Now if I tell you I advocate for software-as-models, they can go a little
crazy on model definition languages. I'm no fan of massive great
XML schema definitions, especially when the used bit is such a small
part of the total bit.For example, anyone remember that 900 pages of XML 
specification proposed by Microsoft? Or 


[.text-center]
artifacct -> notes -> expecation -> test cases -> testing -> issues

=== Lists Upon Lists

TIP: If you want the output to look familiar, copy (or link) the AsciiDoc stylesheet, asciidoc.css, to the output directory.

NOTE: Items marked with TODO are either not yet supported or a work in progress.

To answr that wuqation I wokred with NASA's
independent softare valdation and vertiiation cernter for a decade. 
Unlike normal testing, where the testers live in the same team as the develoersm
IVV ianalysts wr=ork indenepentntly which usually means remotely
to the development team and with much less information abut the productis different to nornak testing-- the "indepenntn" part of IVV means
that the testers are not part of the devleopment tea,
I watched analyss struggling to understand, then add value to, a wide range of
softwar types built in very many ways by developers with widely
varying skills. A common theme in that V&V 

