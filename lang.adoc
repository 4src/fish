= SE+AI, the important BITS (Big Ideas, Tiny Scrips)
Tim Menzies <timm@ieee.org>
:sectanchors: 
:url-repo: https://my-git-repo.com 
:stem: latexmath

We all know how complex and difficult
and expensive it can be to build and maitntain
and understand  
complex software and AI software can be. But not everything
is that hard. Suppose we ask what is the _least_ we can do to
achieve the _most_, what do we get? Now  that we have decades
of experience with AI and SE, what have we learned?
What are the big ideas that lead to the smallest scripts
that do the most?


One big idea, that is often missing is "sampling".
Sampling has a formal definition(footnote{fn-sample}) but I like to call it
"not looking at everything". Life is short, the road is long,
you stop to look at everything or you will never get 
anything done(footnote{fn-maths}).

:fn-sample: footnote:[Approximate a larger population on 
characteristics relevant to the research question, to be 
representative so that researchers can make inferences 
about the larger population. From the 
https://www.nlm.nih.gov/nichsr/stats_tutorial/section2/mod1_sampling.html[National Library of Medicine]]

:fn-maths: footnote:[The are  (stem:[10^{24}]) stars in the observable univerise. 
Say you build software with variables that take four values: 
veryLow, low, hi, veryHigh_. If you software has more that four dozen
variable variables then your software has more states stars
in the sky (stem:[4^{48} > 10^{28}]).]
:fn-notrational: footnote:[For an illuminating, and perhaps humbling
experience, just look at "list of human cognitive biases" in Wikipedia.
At last count, 125 (and growing). This page lists all the ways humans
routinely act in non-rational manner.]

sampling take sus to confngituve sphyctologu and all the quirks
of the human brain that mean we rountinely do not behave
rationally(footnote{notrational}). adn from there, its just
a quick jump for quirks in one brain, to all the qurijly things
that happen when groups of brains get together. Human herd behaviour is fascinating,
if not a little scary.

ai softare is softare. softwre has bugs. things hwith bugs have 
to be tested. 
But how to test softare, which is a very diverse set of artifacts?
In a cost-effective way? I say the answer is modeling. I say that
when faced with somethig u dont understad


artifacct --> notes --> expecation --> test cases --> testing --> issues

To answr that wuqation I wokred with NASA's
independent softare valdation and vertiiation cernter for a decade. 
Unlike normal testing, where the testers live in the same team as the develoersm
IVV ianalysts wr=ork indenepentntly which usually means remotely
to the development team and with much less information abut the productis different to nornak testing-- the "indepenntn" part of IVV means
that the testers are not part of the devleopment tea,
I watched analyss struggling to understand, then add value to, a wide range of
softwar types built in very many ways by developers with widely
varying skills. A common theme in that V&V 
